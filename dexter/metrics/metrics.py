"""High-level metrics runner for prompt activation scoring and caption evaluation."""

import ast
import os
from typing import List, Optional, Sequence, Tuple

from tqdm import tqdm

from .reasoner import Reasoner
from .activation_score import activation_score


class Metrics:
    """Compute activation scores for prompts and generate caption-based reports."""

    def __init__(
        self,
        class_to_activate: int,
        class_path: str,
        classifier,
        classifier_transforms,
        vlm_id: str,
        llm_id: str,
        api_key: str,
        base_url: str,
        system_prompt_config_path: str,
        reasoner: Optional[Reasoner] = None,
    ):
        self.class_to_activate = class_to_activate
        self.class_path = class_path
        self.classifier = classifier
        self.classifier_transforms = classifier_transforms

        self.reasoner = reasoner or Reasoner(
            vlm_id=vlm_id,
            llm_id=llm_id,
            api_key=api_key,
            base_url=base_url,
            system_prompt_config_path=system_prompt_config_path,
            classifier=self.classifier,
            class_to_activate=self.class_to_activate,
            class_path=self.class_path,
        )

        self.captions: List[str] = []
        self.report: Optional[str] = None

    def _load_predicted_words(self) -> List[Sequence[str]]:
        """Read generated prompt candidates from disk if present."""
        pred_words_path = os.path.join(self.class_path, "pred_words.txt")
        if not os.path.exists(pred_words_path):
            return []

        with open(pred_words_path, "r") as file_handle:
            lines = file_handle.readlines()

        prompts: List[Sequence[str]] = []
        for line in lines:
            try:
                prompts.append(ast.literal_eval(line.strip()))
            except (ValueError, SyntaxError):
                continue
        return prompts

    def _render_prompt(self, words: Sequence[str]) -> str:
        """Human-friendly serialization of a tuple of predicted words.

        Args:
            words: Sequence of predicted tokens (length varies by mask type).

        Returns:
            Prompt string in the form "a picture of a X with Y and Z...".
        """
        prompt = "a picture of a "
        for idx, word in enumerate(words):
            if idx == 0:
                prompt += word
            elif idx == 1:
                prompt += f" with {word}"
            else:
                prompt += f" and {word}"
        return prompt

    def _score_prompt(self, prompt: str) -> float:
        """Forward a prompt through the classifier and return the activation score.

        Args:
            prompt: Free-form text prompt to evaluate.

        Returns:
            Activation score (fraction of correct predictions across samples).
        """
        score, _, _ = activation_score(
            classifier=self.classifier,
            prompt=prompt,
            class_to_activate=self.class_to_activate,
            transforms=self.classifier_transforms,
        )
        return score

    def _persist_scores(self, scores: List[Tuple[str, float]], filename: str, header: str) -> None:
        """Write scored prompts to disk with the best entry at the top."""
        output_path = os.path.join(self.class_path, filename)
        best_prompt, best_score = max(scores, key=lambda item: item[1])

        with open(output_path, "w") as file_handle:
            file_handle.write(f"{header}: {best_prompt} - {best_score}\n")
            for prompt, score in scores:
                file_handle.write(f"{prompt} - {score}\n")

    def opt_prompts_act_score(self) -> None:
        """Score prompts generated by DEXTER's optimization loop."""
        prompts_words = self._load_predicted_words()
        if not prompts_words:
            return

        scores: List[Tuple[str, float]] = []
        for words in tqdm(prompts_words, desc="Computing activation score for DEXTER's prompts"):
            prompt = self._render_prompt(words)
            scores.append((prompt, self._score_prompt(prompt)))

        self._persist_scores(scores, filename="as_dexter_prompts.txt", header="Best Activation Score Prompt")

    def captions_act_score(self) -> None:
        """Score captions produced by the reasoning component."""
        scores: List[Tuple[str, float]] = []
        for caption in tqdm(self.captions, desc="Computing activation score for captions"):
            scores.append((caption, self._score_prompt(caption)))

        if not scores:
            return

        self._persist_scores(scores, filename="as_captions.txt", header="Best Activation Score Caption")

    def compute(self) -> None:
        """Full metrics flow: score prompts, generate captions, then score captions."""
        print("Computing activation scores for DEXTER's prompts...")
        self.opt_prompts_act_score()

        print(f"Generating captions and report for {self.class_to_activate}")
        self.captions, self.report = self.reasoner.reason()

        print("Computing activation scores for captions...")
        self.captions_act_score()


if __name__ == "__main__":
    class_folder = "out/292_20251128-170559-babd0823"
    vlm_id = "llava:13b"
    llm_id = "gpt-oss:120b"
    api_key = "your_api_key"
    base_url = "http://localhost:11434/v1"
    system_prompt_config_path = "sp_config"
    class_to_activate = 292

    from dexter import build_classifier_and_transforms

    classifier, transforms = build_classifier_and_transforms(
        clf="robust50", device="cuda", use_tfms=True
    )

    metrics = Metrics(
        class_to_activate=class_to_activate,
        class_path=class_folder,
        classifier=classifier,
        vlm_id=vlm_id,
        llm_id=llm_id,
        api_key=api_key,
        base_url=base_url,
        system_prompt_config_path=system_prompt_config_path,
        classifier_transforms=transforms,
    )
    metrics.compute()
